{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EalVoEReYM2t",
        "outputId": "a9828fa2-9b1a-4f4f-974f-c323aac84e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "# uses of nltk\n",
        "# 1. Tokenization\n",
        "# 2. Stopword removal\n",
        "# 3. Stemming\n",
        "# 4. Lemmatization\n",
        "\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize  # for tokenization\n",
        "from nltk.corpus import stopwords  # for removing stopwords\n",
        "from nltk.stem import PorterStemmer  # for stemming\n",
        "from nltk.stem import WordNetLemmatizer # for lemmatization"
      ],
      "metadata": {
        "id": "KVothsC7YVid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"I am learning Python programming, and it is very helpful!!!!<>\"\n",
        "print(\"Original Text: \",text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmvQpWIzaZG3",
        "outputId": "f914b1cd-382b-474a-e02e-b8b272b897af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  I am learning Python programming, and it is very helpful!!!!<>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-1: Lowercase\n",
        "text=text.lower()\n",
        "print(\"Lowercase Text: \",text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbSbQrklax5N",
        "outputId": "136a844d-2ed6-4ba3-896c-f9c098c19606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercase Text:  i am learning python programming, and it is very helpful!!!!<>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-2: Tokenization\n",
        "# text.split() nhi use krenge , wajah print krke dekh lo aur compare kr lo niche waal se\n",
        "tokens=word_tokenize(text)\n",
        "print(\"Tokenized Text: \",tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kREHm76Pa4yv",
        "outputId": "487e67f7-8855-41f8-bc3e-a2d68da43602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Text:  ['i', 'am', 'learning', 'python', 'programming', ',', 'and', 'it', 'is', 'very', 'helpful', '!', '!', '!', '!', '<', '>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-3: Remove punctuations\n",
        "import string\n",
        "punc=string.punctuation\n",
        "print(punc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_J5MTTYbbFk",
        "outputId": "a385dd06-9f46-43dc-9cd0-55c1e434365d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation_filter = [word for word in tokens if word not in punc]\n",
        "print(\"Removed Punctuations: \",punctuation_filter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxyQPIQObrFF",
        "outputId": "22113aec-ec13-4453-b383-46639717ff38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed Punctuations:  ['i', 'am', 'learning', 'python', 'programming', 'and', 'it', 'is', 'very', 'helpful']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-4: Remove stopwords\n",
        "eng_stopwords=stopwords.words(\"english\")\n",
        "filtered_tokens=[word for word in punctuation_filter if word not in eng_stopwords]\n",
        "print(\"Removed stopwords: \",filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLIOGt1db9Wt",
        "outputId": "a9a77326-72f1-484c-d2d9-a9fe4282085b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed stopwords:  ['learning', 'python', 'programming', 'helpful']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-5: Stemming and Lemmatization\n",
        "stem= PorterStemmer()\n",
        "stem.stem(\"went\") # \"went\" ka \"go\" ho jana chahiye tha lekin nhi hua inhi ki wajah se ham isse use nhi krte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VMCVD0YGcfSe",
        "outputId": "a3d89c5d-a99f-4490-fba2-fe3f7d321ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'went'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wnet= WordNetLemmatizer()\n",
        "print(wnet.lemmatize(\"playing\",\"v\"))\n",
        "print(wnet.lemmatize(\"went\",\"v\"))\n",
        "print(wnet.lemmatize(\"Bought\",\"v\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0IgOhb6d2Z0",
        "outputId": "8a8161a1-ac9d-4596-fb1e-a00f39bd16e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n",
            "go\n",
            "Bought\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words=[]\n",
        "for word in filtered_tokens:\n",
        "  lemmatized_words.append(wnet.lemmatize(word,\"v\"))\n",
        "print(\"Lemmatization: \", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aNMrArLeZDM",
        "outputId": "c8f7d406-1bc4-4751-bd8f-36e6a8f13f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization:  ['learn', 'python', 'program', 'helpful']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text=\" \".join(lemmatized_words)\n",
        "print(\"Cleaned Text:\",cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLRrt-hefBoU",
        "outputId": "375791c0-e63c-4752-cfa0-fce7226fc30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text: learn python program helpful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z5KIVyzHfUi_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}